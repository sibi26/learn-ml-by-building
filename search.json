[
  {
    "objectID": "lectures/01-overview.html",
    "href": "lectures/01-overview.html",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "",
    "text": "Welcome to Machine Learning! This course takes a unique ‚ÄúLearn by Building‚Äù approach where you‚Äôll implement real ML systems from day one. In today‚Äôs hands-on session, we‚Äôll explore the three fundamental paradigms of machine learning‚Äîsupervised, unsupervised, and active learning‚Äîby building CatShop, an e-commerce system that thinks like a cat using Google‚Äôs Gemma-3 language model.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/01-overview.html#overview",
    "href": "lectures/01-overview.html#overview",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "",
    "text": "Welcome to Machine Learning! This course takes a unique ‚ÄúLearn by Building‚Äù approach where you‚Äôll implement real ML systems from day one. In today‚Äôs hands-on session, we‚Äôll explore the three fundamental paradigms of machine learning‚Äîsupervised, unsupervised, and active learning‚Äîby building CatShop, an e-commerce system that thinks like a cat using Google‚Äôs Gemma-3 language model.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/01-overview.html#learning-objectives",
    "href": "lectures/01-overview.html#learning-objectives",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will:\n\nExperience all three ML paradigms through hands-on implementation\nFine-tune Gemma-3 (270M parameters) using LoRA for efficient adaptation\nObserve catastrophic forgetting and model trade-offs\nBuild a complete ML application from data to deployment\nUnderstand how active learning reduces labeling costs",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/01-overview.html#materials",
    "href": "lectures/01-overview.html#materials",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nPre-Lecture Setup\nCourse Overview\nCatShop Notebook\nOpen in Colab\nLecture Folder\n\n\n\n\n\n\n\n\nPre-Class Requirements\n\n\n\n\n\nComplete Pre-Lecture 1 Setup before the lecture!\n\n\n\n\n\n\n\n\n\nReminder\n\n\n\n\n\nJoin the course Piazza forum (Required for participation)\nComplete the Project Matchmaker Form by Mon 8/26, 12:00 PM ET. Required for Lecture 2: k-NN; counts toward participation.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/01-overview.html#interactive-demo",
    "href": "lectures/01-overview.html#interactive-demo",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "Interactive Demo",
    "text": "Interactive Demo\n\n\n\n\n\n\nüê± CatShop Web Application\n\n\n\nExperience your trained model in action with the CatShop e-commerce demo, featuring real-time cat perspective classifications, confidence visualization, and interactive chat.\nBuilt upon the WebShop environment with modifications for educational purposes.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/01-overview.html#acknowledgments-resources",
    "href": "lectures/01-overview.html#acknowledgments-resources",
    "title": "Lecture 1: Welcome to Machine Learning",
    "section": "Acknowledgments & Resources",
    "text": "Acknowledgments & Resources\n\nCore Technologies\n\nGemma-3 270M: Google‚Äôs efficient language model\nPEFT/LoRA: Parameter-efficient fine-tuning\nTransformers: Hugging Face‚Äôs model library\n\n\n\nDataset & Environment\n\nWebShop: E-commerce product data and environment\nYao et al., ‚ÄúWebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents‚Äù, NeurIPS 2022\n\n\nNext: Lecture 2: k-Nearest Neighbors ‚Üí",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L1: Welcome to ML"
    ]
  },
  {
    "objectID": "lectures/00-setup.html",
    "href": "lectures/00-setup.html",
    "title": "Lecture 0: Environment Setup",
    "section": "",
    "text": "Set up Python environment for ML development\nUse Jupyter notebooks effectively\nRun code both locally and on Google Colab\nNavigate course GitHub repository",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L0: Environment Setup"
    ]
  },
  {
    "objectID": "lectures/00-setup.html#learning-objectives",
    "href": "lectures/00-setup.html#learning-objectives",
    "title": "Lecture 0: Environment Setup",
    "section": "",
    "text": "Set up Python environment for ML development\nUse Jupyter notebooks effectively\nRun code both locally and on Google Colab\nNavigate course GitHub repository",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L0: Environment Setup"
    ]
  },
  {
    "objectID": "lectures/00-setup.html#materials",
    "href": "lectures/00-setup.html#materials",
    "title": "Lecture 0: Environment Setup",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nSetup Notebook\nOpen in Colab\nLecture Folder\n\n\n\n\n\n\n\n\nReminder\n\n\n\n\n\nJoin the course Piazza forum (Required for participation)\nComplete the Project Matchmaker Form by Tue 8/26, 12:00 PM ET. Required for Lecture 2: k-NN; counts toward participation.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L0: Environment Setup"
    ]
  },
  {
    "objectID": "lectures/00-setup.html#pre-class-preparation",
    "href": "lectures/00-setup.html#pre-class-preparation",
    "title": "Lecture 0: Environment Setup",
    "section": "Pre-Class Preparation",
    "text": "Pre-Class Preparation\n\nComplete environment setup: Setup Guide (SETUP.md)\nRun through the setup notebook\nPost any issues on Piazza\n\n\nNext: Lecture 1: Welcome to Machine Learning ‚Üí",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L0: Environment Setup"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html",
    "href": "lectures/04-optimization.html",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "",
    "text": "Gradient descent is the fundamental algorithm that makes modern machine learning possible. In this comprehensive lecture, we‚Äôll journey from manually finding the ‚Äúbest line‚Äù through interactive widgets to implementing gradient descent from scratch, exploring SGD variants, and mastering debugging techniques. Using the California Housing dataset, you‚Äôll build deep intuition about how optimization works, why data standardization is critical, and how to diagnose and fix common training failures.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html#overview",
    "href": "lectures/04-optimization.html#overview",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "",
    "text": "Gradient descent is the fundamental algorithm that makes modern machine learning possible. In this comprehensive lecture, we‚Äôll journey from manually finding the ‚Äúbest line‚Äù through interactive widgets to implementing gradient descent from scratch, exploring SGD variants, and mastering debugging techniques. Using the California Housing dataset, you‚Äôll build deep intuition about how optimization works, why data standardization is critical, and how to diagnose and fix common training failures.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html#learning-objectives",
    "href": "lectures/04-optimization.html#learning-objectives",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will:\n\nFormulate optimization problems in machine learning\nImplement gradient descent from scratch with proper numerical stability\nMaster the critical importance of data standardization\nCompare batch, stochastic, and mini-batch gradient descent variants\nDiagnose and fix gradient explosion/vanishing problems\nApply modern optimizers (Adam, SGD with momentum) effectively",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html#materials",
    "href": "lectures/04-optimization.html#materials",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nOptimization Notebook\n\n\n\n\n\n\n\n\nPre-Class Requirements\n\n\n\n\n\n\nPython environment with NumPy, scikit-learn, matplotlib, and PyTorch\nBasic calculus knowledge (derivatives, chain rule)\nFamiliarity with linear regression concepts from Lecture 3",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html#key-concepts-highlights",
    "href": "lectures/04-optimization.html#key-concepts-highlights",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "Key Concepts & Highlights",
    "text": "Key Concepts & Highlights\n\nInteractive Line Fitting Game Start by manually adjusting weights and biases to minimize loss. This hands-on experience builds intuition for what gradient descent automates‚Äîfinding the lowest point in the loss landscape.\nThe #1 Pitfall: Data Standardization Learn why unstandardized data causes gradient explosion and how proper preprocessing prevents this. We‚Äôll demonstrate why gradients scale with data magnitude and provide diagnostic tools to catch this early.\nThree Flavors of SGD Compare batch, stochastic, and mini-batch gradient descent side-by-side. Understand the speed vs.¬†accuracy trade-offs and why mini-batch has become the industry standard.\nDebugging Toolkit Master systematic debugging with our gradient diagnostic dashboard that detects vanishing/exploding gradients, loss plateaus, and oscillations‚Äîcomplete with suggested fixes for each problem.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "lectures/04-optimization.html#datasets-acknowledgments",
    "href": "lectures/04-optimization.html#datasets-acknowledgments",
    "title": "Lecture 4: Gradient Descent and Optimization",
    "section": "Datasets & Acknowledgments",
    "text": "Datasets & Acknowledgments\n\nCalifornia Housing Dataset\n\nSource: Scikit-learn California housing dataset- Why This Dataset: Strong income‚Äìprice correlation makes it perfect for demonstrating optimization concepts while being complex enough to show real challenges\n\n\nPrevious: ‚Üê Lecture 3: Linear Regression | Next: Lecture 5: Probabilistic Classification",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L4: Gradient Descent and Optimization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning: Learn by Building",
    "section": "",
    "text": "This course takes a hands-on approach to machine learning. You‚Äôll:\n\nBuild real ML systems, not just study theory\nCreate an AI portfolio of mini research papers\nExplain complex concepts simply (teaching = understanding)\nWork with cutting-edge tools while mastering fundamentals\n\nWe believe the best way to understand machine learning is to build it, break it, and rebuild it better.\nOur practical, project-first ethos is inspired by the excellent work from fast.ai.\nCourse Philosophy: Inspired by fast.ai, our motto is ‚Äúbuild first, understand later.‚Äù You‚Äôll start by building and fine-tuning powerful models, and then we‚Äôll dive deep into the theory to understand why they work.\nA Note on AI in Course Development: In the spirit of transparency, I want to acknowledge that AI-powered tools were used to help develop these course materials, from refining explanations to generating boilerplate code. This is to model the professional practice of using the best tools available to produce a high-quality result. Your goal in this course is different: it‚Äôs to learn the fundamental process yourself. Our course policies will reflect this important distinction."
  },
  {
    "objectID": "index.html#course-philosophy",
    "href": "index.html#course-philosophy",
    "title": "Machine Learning: Learn by Building",
    "section": "",
    "text": "This course takes a hands-on approach to machine learning. You‚Äôll:\n\nBuild real ML systems, not just study theory\nCreate an AI portfolio of mini research papers\nExplain complex concepts simply (teaching = understanding)\nWork with cutting-edge tools while mastering fundamentals\n\nWe believe the best way to understand machine learning is to build it, break it, and rebuild it better.\nOur practical, project-first ethos is inspired by the excellent work from fast.ai.\nCourse Philosophy: Inspired by fast.ai, our motto is ‚Äúbuild first, understand later.‚Äù You‚Äôll start by building and fine-tuning powerful models, and then we‚Äôll dive deep into the theory to understand why they work.\nA Note on AI in Course Development: In the spirit of transparency, I want to acknowledge that AI-powered tools were used to help develop these course materials, from refining explanations to generating boilerplate code. This is to model the professional practice of using the best tools available to produce a high-quality result. Your goal in this course is different: it‚Äôs to learn the fundamental process yourself. Our course policies will reflect this important distinction."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Machine Learning: Learn by Building",
    "section": "Getting Started",
    "text": "Getting Started\n\nSet up your environment ‚Üí Setup Guide\nJoin Piazza for discussions and announcements\nReview the syllabus ‚Üí Course Policies\nCheck the schedule ‚Üí Week-by-Week"
  },
  {
    "objectID": "index.html#course-team",
    "href": "index.html#course-team",
    "title": "Machine Learning: Learn by Building",
    "section": "Course Team",
    "text": "Course Team\nInstructor: Ming Jin\nEmail: jinming@vt.edu\n\n\n\n\n\n\n\n\n\nTeaching Assistant\nLocation\nOffice Hours\nEmail\n\n\n\n\nSherawat, Kamal\nWhittemore 256\nTue/Thu 11:00 AM‚Äì12:00 Noon\nkamals@vt.edu\n\n\nNere, Darshan\nWhittemore 256\nTue/Thu 3:00‚Äì4:00 PM\ndarshannere@vt.edu"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "For the most up-to-date and complete syllabus, please refer to the official Google Doc:\n\nSyllabus (published): https://docs.google.com/document/d/e/2PACX-1vRawfzMzEKCan7nd1jOMbaCGrcMYo-DGU1XgfIBKJSRBiuF_Fj0E4KCeslTIGzRs5R60XAsYvURItxv/pub\n\nAny updates will appear in the Google Doc.",
    "crumbs": [
      "Home",
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#official-syllabus",
    "href": "syllabus.html#official-syllabus",
    "title": "Course Syllabus",
    "section": "",
    "text": "For the most up-to-date and complete syllabus, please refer to the official Google Doc:\n\nSyllabus (published): https://docs.google.com/document/d/e/2PACX-1vRawfzMzEKCan7nd1jOMbaCGrcMYo-DGU1XgfIBKJSRBiuF_Fj0E4KCeslTIGzRs5R60XAsYvURItxv/pub\n\nAny updates will appear in the Google Doc.",
    "crumbs": [
      "Home",
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#capstone-project-rubrics",
    "href": "syllabus.html#capstone-project-rubrics",
    "title": "Course Syllabus",
    "section": "Capstone Project Rubrics",
    "text": "Capstone Project Rubrics\nRubrics for the capstone project are available here:\n\nCapstone Rubrics (Google Doc): https://docs.google.com/document/d/19t7HrBbnLVrmRtOs5qiSXxERDdMcxxKG-LZQqSTZmJs/edit?usp=sharing\n\nIf you need a printable or offline copy, use File ‚Üí Download in Google Docs.",
    "crumbs": [
      "Home",
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Python Installation\nGoogle Colab\nGitHub Student Pack"
  },
  {
    "objectID": "resources.html#software-tools",
    "href": "resources.html#software-tools",
    "title": "Resources",
    "section": "",
    "text": "Python Installation\nGoogle Colab\nGitHub Student Pack"
  },
  {
    "objectID": "resources.html#documentation",
    "href": "resources.html#documentation",
    "title": "Resources",
    "section": "Documentation",
    "text": "Documentation\n\nNumPy Documentation\nPyTorch Tutorials\nfast.ai ‚Äî inspiration for our practical, project-first teaching approach\nScikit-learn User Guide"
  },
  {
    "objectID": "projects/project1.html",
    "href": "projects/project1.html",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "",
    "text": "Build a Spotify-inspired music recommendation engine from scratch, implementing the k-Nearest Neighbors algorithm and deploying it as an interactive web application. This project bridges the gap between academic ML and real-world applications, taking you through the complete pipeline from data exploration to production deployment. You‚Äôll work with actual Spotify audio features to recommend songs based on musical similarity.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#overview",
    "href": "projects/project1.html#overview",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "",
    "text": "Build a Spotify-inspired music recommendation engine from scratch, implementing the k-Nearest Neighbors algorithm and deploying it as an interactive web application. This project bridges the gap between academic ML and real-world applications, taking you through the complete pipeline from data exploration to production deployment. You‚Äôll work with actual Spotify audio features to recommend songs based on musical similarity.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#learning-objectives",
    "href": "projects/project1.html#learning-objectives",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy completing this project, you will:\n\nImplement k-NN from scratch with multiple distance metrics (Euclidean, Cosine)\nAnalyze Spotify‚Äôs audio features (energy, valence, danceability) through exploratory data analysis\nEngineer features and apply scaling techniques to handle multi-scale data\nDesign hybrid distance functions that combine audio features with metadata\nEvaluate computational complexity and scalability challenges\nDeploy your ML model as a REST API with a responsive web interface\nOptimize with caching strategies for production performance",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#materials",
    "href": "projects/project1.html#materials",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nProject Notebook (GitHub) ‚Äî Complete implementation guide and assignment\nRepository ‚Äî Full codebase with web application\nLive Demo (Static UI) ‚Äî UI preview only (no backend)\nLive Demo ‚Äî after running python app.py\n\n\n\n\n\n\n\n\nSubmission Requirements\n\n\n\n\n\n\nCompleted Jupyter notebook exported as PDF\nutils/student_adapter.py with your implementations\nWorking web application demonstration",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#what-youll-build",
    "href": "projects/project1.html#what-youll-build",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "What You‚Äôll Build",
    "text": "What You‚Äôll Build\n\nCore Algorithm\nYou‚Äôll implement k-NN from the ground up, including: - Distance metric calculations (Euclidean and Cosine) - Efficient neighbor search - Feature scaling for fair comparisons - Hybrid distance functions combining multiple signals\n\n\nWeb Application\nYour algorithm will power a full-stack music discovery application featuring: - Search for any track in a 600k+ song database - Real-time recommendations based on audio similarity - YouTube integration for music previews - Interactive controls for algorithm parameters - Mobile-responsive design\n\n\nData Analysis\nWorking with real Spotify data, you‚Äôll explore: - 9 audio features extracted by Spotify‚Äôs ML models - Feature distributions, correlations, and outliers - The impact of scaling on distance-based algorithms - Trade-offs between different similarity measures",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#key-technologies",
    "href": "projects/project1.html#key-technologies",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Key Technologies",
    "text": "Key Technologies\n\nBackend: Flask, NumPy, Pandas, Scikit-learn\nFrontend: Vanilla JavaScript, HTML5, CSS3\nAPIs: YouTube Data API for music previews\nData: 600k+ tracks with Spotify audio analysis",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#interactive-features",
    "href": "projects/project1.html#interactive-features",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Interactive Features",
    "text": "Interactive Features\n\n\n\n\n\n\nüéµ Try It Yourself\n\n\n\nOnce deployed, your system will let users:\n\nSearch and discover music from over half a million tracks\nGet personalized recommendations based on any song\nAdjust algorithm parameters (k, distance metrics) in real-time\nListen to preview clips before adding to playlists\nUnderstand why songs were recommended through feature visualization",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#setup-guide",
    "href": "projects/project1.html#setup-guide",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Setup Guide",
    "text": "Setup Guide\n# Quick setup\npython3.11 -m venv .venv-music\nsource .venv-music/bin/activate\npip install -r requirements.txt\n\n# Configure YouTube API (see notebook for detailed instructions)\necho \"YOUTUBE_API_KEY=your_key_here\" &gt; .env\n\n# Run the application\npython app.py\n# Navigate to http://localhost:5002",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#data-setup",
    "href": "projects/project1.html#data-setup",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Data Setup",
    "text": "Data Setup\nPlace the dataset CSV(s) in the project‚Äôs data/ folder:\n\nRequired: data/mergedFile.csv ‚Äî main catalog with track metadata and audio features\nOptional: data/item_profile.csv ‚Äî alternative profile file if you choose to separate it\n\nNotes:\n\nColumn expectations include song/artist identifiers and audio features (e.g., energy, danceability, etc.).\nThe application won‚Äôt function without mergedFile.csv. See repository README for more details and dataset link.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#real-world-challenges-youll-tackle",
    "href": "projects/project1.html#real-world-challenges-youll-tackle",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Real-World Challenges You‚Äôll Tackle",
    "text": "Real-World Challenges You‚Äôll Tackle\n\nThe Curse of Dimensionality: Understand why distance metrics behave unexpectedly in high dimensions\nFeature Scale Bias: See how tempo (0-200 BPM) can dominate valence (0-1) without proper scaling\nComputational Complexity: Analyze why brute-force k-NN doesn‚Äôt scale to millions of songs\nProduction Engineering: Implement caching and API design for responsive user experience\nHybrid Metrics: Balance audio similarity with artist diversity for better recommendations",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#project-insights",
    "href": "projects/project1.html#project-insights",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Project Insights",
    "text": "Project Insights\nThis project demonstrates several key ML concepts:\n\nInstance-based learning requires no training phase but trades computation at query time\nFeature engineering can be more important than algorithm choice\nDistance metrics encode our notion of similarity - choosing the right one is crucial\nProduction ML involves much more than just the algorithm - APIs, caching, and UX matter",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "projects/project1.html#additional-resources",
    "href": "projects/project1.html#additional-resources",
    "title": "Project 1: Music Recommendation System with k-NN",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSpotify Web API - Audio Features Documentation\nk-NN Algorithm Explained\nFlask Quick Start Guide\nCurse of Dimensionality in ML\nRUMusic - Conceptual inspiration for this project (our implementation has significantly diverged)\n\n\nPrevious: [‚Üê Previous Assignment] | Next: [Next Assignment ‚Üí]",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1: KNN Music Recommender"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Live Schedule\n\n\n\nThe schedule below is embedded from Google Sheets and updates automatically. Open in Google Sheets ‚Üí",
    "crumbs": [
      "Home",
      "Course Info",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#week-by-week-schedule",
    "href": "schedule.html#week-by-week-schedule",
    "title": "Course Schedule",
    "section": "",
    "text": "Live Schedule\n\n\n\nThe schedule below is embedded from Google Sheets and updates automatically. Open in Google Sheets ‚Üí",
    "crumbs": [
      "Home",
      "Course Info",
      "Schedule"
    ]
  },
  {
    "objectID": "lectures/03-linear.html",
    "href": "lectures/03-linear.html",
    "title": "Lecture 3: Linear Regression",
    "section": "",
    "text": "Linear regression is far more powerful than its name suggests. In this hands-on lecture, we‚Äôll discover how the same algorithm that fits straight lines can capture complex curved relationships in real-world data. Through interactive experiments with automotive engineering and weather forecasting datasets, you‚Äôll master polynomial features, understand the ‚Äúlifting technique,‚Äù and learn when models become too complex.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L3: Linear Regression"
    ]
  },
  {
    "objectID": "lectures/03-linear.html#overview",
    "href": "lectures/03-linear.html#overview",
    "title": "Lecture 3: Linear Regression",
    "section": "",
    "text": "Linear regression is far more powerful than its name suggests. In this hands-on lecture, we‚Äôll discover how the same algorithm that fits straight lines can capture complex curved relationships in real-world data. Through interactive experiments with automotive engineering and weather forecasting datasets, you‚Äôll master polynomial features, understand the ‚Äúlifting technique,‚Äù and learn when models become too complex.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L3: Linear Regression"
    ]
  },
  {
    "objectID": "lectures/03-linear.html#learning-objectives",
    "href": "lectures/03-linear.html#learning-objectives",
    "title": "Lecture 3: Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will:\n\nUnderstand linear regression model, parameters, and MSE loss function\nMaster polynomial features through hands-on interaction\nApply regression to real problems (automotive MPG, weather bias correction)\nVisualize the ‚Äúlifting technique‚Äù that transforms curves into hyperplanes\nRecognize overfitting and understand model complexity trade-offs\nEngineer domain-specific features that outperform blind polynomial expansion",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L3: Linear Regression"
    ]
  },
  {
    "objectID": "lectures/03-linear.html#materials",
    "href": "lectures/03-linear.html#materials",
    "title": "Lecture 3: Linear Regression",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nLinear Regression Notebook\nOpen in Colab\nLecture Folder\n\n\n\n\n\n\n\n\nPre-Class Requirements\n\n\n\n\n\nComplete the Environment Setup Guide.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L3: Linear Regression"
    ]
  },
  {
    "objectID": "lectures/03-linear.html#datasets-acknowledgments",
    "href": "lectures/03-linear.html#datasets-acknowledgments",
    "title": "Lecture 3: Linear Regression",
    "section": "Datasets & Acknowledgments",
    "text": "Datasets & Acknowledgments\n\nReal-World Datasets Used\n\nAuto MPG Dataset: 398 vehicles from 1970-1982\nUCI Machine Learning Repository, accessed via MLxtend\nECMWF Weather Data: 5.2M observations from 8,000+ weather stations\nEuropean Centre for Medium-Range Weather Forecasts, via ClimeLab (Apache 2.0)\n\n\nPrevious: ‚Üê Lecture 2: k-NN | Next: Lecture 4: Gradient Descent and Optimization ‚Üí",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L3: Linear Regression"
    ]
  },
  {
    "objectID": "lectures/02-knn.html",
    "href": "lectures/02-knn.html",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "",
    "text": "In this hands-on lecture, we tackle a real problem: finding compatible ML project teammates using k-Nearest Neighbors. Yes, you‚Äôll actually use this system to form your project teams! We‚Äôll explore how to frame real-world problems as ML tasks, understand the elegance of lazy learning, and confront the challenges of high-dimensional spaces.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#overview",
    "href": "lectures/02-knn.html#overview",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "",
    "text": "In this hands-on lecture, we tackle a real problem: finding compatible ML project teammates using k-Nearest Neighbors. Yes, you‚Äôll actually use this system to form your project teams! We‚Äôll explore how to frame real-world problems as ML tasks, understand the elegance of lazy learning, and confront the challenges of high-dimensional spaces.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#learning-objectives",
    "href": "lectures/02-knn.html#learning-objectives",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will:\n\nFrame real-world problems as ML tasks with inputs (X) and outputs (y)\nApply k-NN for classification and regression tasks\nAnalyze the impact of distance metrics and feature scaling\nUnderstand the curse of dimensionality and its practical implications\nDesign fair and effective matching systems with domain constraints\nEvaluate trade-offs between different similarity measures",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#materials",
    "href": "lectures/02-knn.html#materials",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nLecture Notebook\nOpen in Colab\nLecture Folder\n\n\n\n\n\n\n\n\nPre-Class Requirements\n\n\n\n\n\nComplete the Project Matchmaker Form by Mon 8/26, 12:00 PM ET. Required for Lecture 2: k-NN; counts toward participation.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#interactive-demo",
    "href": "lectures/02-knn.html#interactive-demo",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "Interactive Demo",
    "text": "Interactive Demo\n\n\n\n\n\n\nüéÆ Team Matcher Visualization\n\n\n\nExperience k-NN in action with our Interactive Team Matching Demo\nThis real-time visualization lets you:\n\nAdjust k parameter and see immediate effects\nExplore different dimension pairs\nView how proximity translates to similarity\nDiscover natural clustering patterns in the class",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#key-topics",
    "href": "lectures/02-knn.html#key-topics",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "Key Topics",
    "text": "Key Topics\n\nThe Data Journey: From text surveys ‚Üí NLP features ‚Üí 8D vectors\nk-NN Fundamentals: The beautiful simplicity of ‚Äúyou are your neighbors‚Äù\nDistance Metrics: Euclidean, Manhattan, Cosine, and when each matters\nCurse of Dimensionality: When all points become equidistant\nFairness in ML: Ensuring inclusive team formation",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/02-knn.html#additional-resources",
    "href": "lectures/02-knn.html#additional-resources",
    "title": "Lecture 2: k-Nearest Neighbors",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nScikit-learn k-NN Documentation\n\n\nPrevious: ‚Üê Lecture 1: Welcome to ML | Next: Lecture 3: Linear Regression ‚Üí",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L2: k-NN"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html",
    "href": "lectures/05-probclass.html",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "",
    "text": "In this critical lecture, we tackle a life-or-death classification problem: identifying poisonous mushrooms. Through this compelling case study, you‚Äôll discover why probabilistic outputs are essential for risk-aware decisions, moving beyond simple yes/no predictions. We‚Äôll build two fundamental probabilistic classifiers‚ÄîLogistic Regression and Naive Bayes‚Äîand learn how to calibrate them for safety-critical applications where the cost of false negatives can be fatal.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html#overview",
    "href": "lectures/05-probclass.html#overview",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "",
    "text": "In this critical lecture, we tackle a life-or-death classification problem: identifying poisonous mushrooms. Through this compelling case study, you‚Äôll discover why probabilistic outputs are essential for risk-aware decisions, moving beyond simple yes/no predictions. We‚Äôll build two fundamental probabilistic classifiers‚ÄîLogistic Regression and Naive Bayes‚Äîand learn how to calibrate them for safety-critical applications where the cost of false negatives can be fatal.",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html#learning-objectives",
    "href": "lectures/05-probclass.html#learning-objectives",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will:\n\nUnderstand why probabilistic outputs are essential for risk-aware decisions\nImplement logistic regression as a ‚Äúsoft‚Äù decision maker that expresses confidence\nMaster Naive Bayes as a probabilistic detective gathering and combining evidence\nCalibrate models and tune decision thresholds for safety-critical applications\nDiagnose model disagreements and understand edge cases\nBuild production-ready classifiers that prioritize safety over accuracy",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html#materials",
    "href": "lectures/05-probclass.html#materials",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nQuick Access\n\n\n\nProbabilistic Classification Notebook\n\n\n\n\n\n\n\n\nPre-Class Requirements\n\n\n\n\n\n\nPython environment with scikit-learn, pandas, matplotlib, and seaborn\nUnderstanding of linear regression from Lecture 3\nBasic probability theory (Bayes‚Äô theorem helpful but not required)\nCompleted Lecture 4 on optimization",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html#datasets-acknowledgments",
    "href": "lectures/05-probclass.html#datasets-acknowledgments",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "Datasets & Acknowledgments",
    "text": "Datasets & Acknowledgments\n\nUCI Mushroom Dataset\n\nSource: UCI Machine Learning Repository\nSize: 8,124 mushrooms with 22 categorical features\nWhy This Dataset: Perfect 52/48 class balance, real mushroom characteristics from field guides, and life-or-death stakes make it ideal for teaching safety-critical classification",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  },
  {
    "objectID": "lectures/05-probclass.html#key-takeaways",
    "href": "lectures/05-probclass.html#key-takeaways",
    "title": "Lecture 5: Probabilistic Classification",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nCritical Safety Principle\n\n\n\nIn mushroom classification, it‚Äôs better to skip a meal than risk your life! This principle extends to all safety-critical applications where false negatives have catastrophic consequences.\n\n\n\nPrevious: ‚Üê Lecture 4: Gradient Descent and Optimization | Next: Lecture 6: Decision Trees (Coming Soon)",
    "crumbs": [
      "Home",
      "Module 1: Let's Predict",
      "L5: Probabilistic Classification"
    ]
  }
]