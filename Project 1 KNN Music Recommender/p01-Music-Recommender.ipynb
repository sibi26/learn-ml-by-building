{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Music Recommendation System with k-NN\n",
    "\n",
    "## ECE4424/CS4824: Machine Learning, Fall 2025\n",
    "**Instructor**: Prof. Ming Jin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Overview\n",
    "**Total Points: 100**\n",
    "- Part 1: Data Exploration - 25 points\n",
    "- Part 2: k-NN Implementation - 35 points  \n",
    "- Part 3: Feature Scaling - 10 points\n",
    "- Part 4: Advanced Similarity & Complexity - 20 points\n",
    "- Part 5: Production Integration - 10 points\n",
    "\n",
    "### Submission Requirements\n",
    "1. Complete ALL code implementations and REPORT SECTIONS in **this notebook**\n",
    "2. Run all cells to show outputs (including plots)\n",
    "3. Export as PDF: File ‚Üí Download as ‚Üí PDF via LaTeX\n",
    "4. Copy your final implementations to `utils/student_adapter.py`\n",
    "5. Verify the web app works with your code\n",
    "6. Submit: (1) This notebook as PDF, (2) student_adapter.py file\n",
    "\n",
    "**Note: REPORT SECTIONS are clearly marked and will be graded for analysis quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Project Overview\n",
    "\n",
    "This assignment is a cornerstone of the **AI Portfolio**, a central component of this course where you will complete a collection of projects designed to build tangible skills and a demonstrable body of work. \n",
    "\n",
    "In this project, you will bridge the gap between academic machine learning and real-world applications by building a music recommendation engine. The process will guide you through a systematic exploration, from foundational data analysis to model implementation and integration, providing insights beyond what a siloed examination of algorithms can offer.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end, you'll have:\n",
    "\n",
    "1. Implemented k-NN from scratch, a fundamental and widely used algorithm.\n",
    "2. Designed and evaluated multiple distance metrics, understanding how they capture different notions of similarity.\n",
    "3. Gained hands-on experience with the critical impact of feature engineering and scaling.\n",
    "4. Understood the steps to integrate a machine learning model into a larger application via an API.\n",
    "5. Gained insights into the practical challenges and advanced concepts in building recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Exploration](#part1)\n",
    "2. [Implementing k-NN from Scratch](#part2)\n",
    "3. [Feature Scaling and Its Importance](#part3)\n",
    "4. [Advanced Similarity & Complexity](#part4)\n",
    "5. [From Model to Web Application](#part5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## Part 1: Setup and Data Exploration (25 points)\n",
    "\n",
    "Before building our recommendation engine, we must understand our data. Spotify's audio analysis provides rich features for each track, extracted using deep learning models and signal processing algorithms. This initial exploration is the most critical step in any applied machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Spotify Audio Features\n",
    "\n",
    "Spotify analyzes every track to extract a rich set of features. Understanding these is key to building an effective model. According to the [official Spotify Web API documentation](https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features), these features include:\n",
    "\n",
    "**Perceptual Features** (How music sounds to humans):\n",
    "- **`energy`** (0-1): Perceptual intensity and activity. Death metal has high energy; a Bach prelude has low energy.\n",
    "- **`valence`** (0-1): Musical positiveness. Happy, cheerful songs score high; sad, angry songs score low.\n",
    "- **`danceability`** (0-1): How suitable a track is for dancing, based on a combination of tempo, rhythm stability, and beat strength.\n",
    "- **`acousticness`** (0-1): A confidence measure from 0.0 to 1.0 of whether the track is acoustic.\n",
    "\n",
    "**Technical Features** (Musical properties):\n",
    "- **`tempo`**: The overall estimated tempo of a track in beats per minute (BPM).\n",
    "- **`loudness`**: The overall loudness of a track in decibels (dB), averaged across the track.\n",
    "- **`key`**: The estimated key of the track. Integers map to pitches using standard Pitch Class notation (e.g., 0 = C, 1 = C‚ôØ/D‚ô≠, 2 = D, and so on).\n",
    "- **`mode`**: Indicates the modality (major or minor) of a track. Major is represented by 1 and minor is 0.\n",
    "\n",
    "**Content Features** (What's in the music):\n",
    "- **`speechiness`** (0-1): Detects the presence of spoken words. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech. Values below 0.33 most likely represent music.\n",
    "- **`instrumentalness`** (0-1): Predicts whether a track contains no vocals. The closer the value is to 1.0, the greater likelihood the track is instrumental.\n",
    "- **`liveness`** (0-1): Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Audio\n",
    "import subprocess\n",
    "import atexit\n",
    "\n",
    "# Configure libraries\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    tracks_df = pd.read_csv('data/mergedFile.csv', dtype={'id': str})\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    tracks_df.dropna(subset=['id', 'song', 'artist'], inplace=True)\n",
    "    print(f\"{len(tracks_df)} tracks available for the assignment.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Make sure you have the 'data' folder with 'mergedFile.csv' in your project root.\")\n",
    "\n",
    "# Define the core audio features we'll use in our model\n",
    "audio_features = ['energy', 'danceability', 'acousticness', 'valence', 'tempo', 'instrumentalness', 'loudness', 'liveness', 'speechiness']\n",
    "required_cols = ['id', 'song', 'artist'] + audio_features\n",
    "missing_cols = [col for col in required_cols if col not in tracks_df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"The loaded dataframe is missing the following required columns: {missing_cols}\")\n",
    "\n",
    "print(\"\\nSample tracks from the dataset (showing all features):\")\n",
    "display(tracks_df[required_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Interactive Data Exploration\n",
    "\n",
    "To build intuition, it helps to listen to the music. The function below creates an interactive player for a small sample of the dataset. Run the cell and listen to a few tracks to get a feel for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube API Setup Instructions\n",
    "\n",
    "To enable audio previews, get a YouTube API key:\n",
    "\n",
    "1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
    "2. Create new project or select existing\n",
    "3. Enable API: \"APIs & Services\" ‚Üí \"Library\" ‚Üí search \"YouTube Data API v3\" ‚Üí Enable\n",
    "4. Create credentials: \"APIs & Services\" ‚Üí \"Credentials\" ‚Üí \"Create Credentials\" ‚Üí \"API Key\"\n",
    "5. Add to `.env` file in project root:\n",
    "\n",
    "   ```\n",
    "   YOUTUBE_API_KEY=your_key_here\n",
    "   ```\n",
    "6. The notebook will automatically load it from `.env`\n",
    "\n",
    "**Note:** We need this for the webapp to load music properly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqq --disable-pip-version-check --no-input --progress-bar off google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Audio previews with YouTube (requires API key in .env)\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "# Load .env from the same directory as the notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "env_path = os.path.join(notebook_dir, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "YOUTUBE_API_KEY = os.environ.get('YOUTUBE_API_KEY')\n",
    "\n",
    "def create_music_player(df, n=5):\n",
    "    \"\"\"Create YouTube embedded players if API key is available.\"\"\"\n",
    "    if not YOUTUBE_API_KEY:\n",
    "        print(\"No YouTube API key found in .env file\")\n",
    "        print(\"Add YOUTUBE_API_KEY=your_key to .env file to enable audio previews\")\n",
    "        return\n",
    "    \n",
    "    from googleapiclient.discovery import build\n",
    "    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "    \n",
    "    samples = df.sample(n)\n",
    "    html = \"<h3>Sample Tracks</h3>\"\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        search = youtube.search().list(\n",
    "            q=f\"{row['song']} {row['artist']}\",\n",
    "            part='snippet',\n",
    "            type='video',\n",
    "            maxResults=1\n",
    "        ).execute()\n",
    "        \n",
    "        if search['items']:\n",
    "            video_id = search['items'][0]['id']['videoId']\n",
    "            html += f\"\"\"\n",
    "            <div style='margin: 10px 0;'>\n",
    "                <b>{row['song']}</b> - {row['artist']}<br>\n",
    "                <iframe width=\"320\" height=\"80\" \n",
    "                        src=\"https://www.youtube.com/embed/{video_id}\"></iframe>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "# Test if API key loaded\n",
    "print(f\"API Key loaded: {'Yes' if YOUTUBE_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show players only if API key exists\n",
    "if YOUTUBE_API_KEY:\n",
    "    create_music_player(tracks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Quality and Feature Distributions\n",
    "\n",
    "A crucial step in any real-world ML project is to perform a thorough data quality analysis. We need to check the **distributions**, **scales**, and **outliers** for each feature. \n",
    "\n",
    "- **Distribution**: Describes how the values of a feature are spread out. Are they symmetric (like a bell curve), or are they skewed to one side?\n",
    "- **Scale**: Refers to the range of values a feature can take (e.g., 0-1 vs. 0-200).\n",
    "- **Outliers**: These are data points that are significantly different from other observations. They can be legitimate extreme values or errors in the data.\n",
    "- **Skewness**: A measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined. For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. A value close to 0 indicates a symmetric distribution.\n",
    "  - Skewness is mathematically defined as the third standardized moment: $g_1 = E\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right]$\n",
    "\n",
    "For more detail on these statistical concepts, you can refer to the [SciPy stats documentation](https://docs.scipy.org/doc/scipy/reference/stats.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** (5 points): A crucial step in any real-world ML project is to perform a thorough data quality analysis. The helper functions below will generate a quality report and visualize the distributions. Review the generated report and plots and complete the following section directly in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 1.1: Distribution Analysis (5 points)\n",
    "**Write your analysis below (100-150 words)**\n",
    "\n",
    "Based on the data quality report and distribution plots:\n",
    "\n",
    "**Features with significant outliers:** Which features have the most significant outliers? (e.g., `speechiness`, `instrumentalness`).\n",
    "\n",
    "**Features requiring transformation:** Based on the skewness values and distribution plots, which features might benefit from a transformation (e.g., a log transform)?\n",
    "\n",
    "**Impact on k-NN:** How might the vast difference in feature scales (e.g., `tempo` vs. `danceability`) and the presence of outliers negatively affect a distance-based algorithm like k-NN? (We'll solve this in Section 4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quality_report(df, features):\n",
    "    \"\"\"Generates a data quality report for the specified features.\"\"\"\n",
    "    report = []\n",
    "    for feature in features:\n",
    "        data = df[feature].dropna()\n",
    "        stats = {\n",
    "            'Feature': feature,\n",
    "            'Mean': data.mean(),\n",
    "            'Median': data.median(),\n",
    "            'Std Dev': data.std(),\n",
    "            'Min': data.min(),\n",
    "            'Max': data.max(),\n",
    "            'Range': data.max() - data.min(),\n",
    "            'Skewness': data.skew(),\n",
    "            'Outliers (>3œÉ)': len(data[np.abs(data - data.mean()) > 3 * data.std()])\n",
    "        }\n",
    "        report.append(stats)\n",
    "    report_df = pd.DataFrame(report).set_index('Feature')\n",
    "    display(report_df.round(2))\n",
    "\n",
    "def visualize_distributions(df, features):\n",
    "    \"\"\"Creates distribution plots (histogram and KDE) for all specified features.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.ravel()\n",
    "    for idx, feature in enumerate(features):\n",
    "        ax = axes[idx]\n",
    "        data = df[feature].dropna()\n",
    "        sns.histplot(data, ax=ax, kde=True, bins=30)\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {data.mean():.2f}\")\n",
    "        ax.axvline(data.median(), color='green', linestyle='-', linewidth=2, label=f\"Median: {data.median():.2f}\")\n",
    "        ax.set_title(f'Distribution of {feature.capitalize()}', fontsize=14)\n",
    "        ax.set_xlabel('')\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis\n",
    "create_quality_report(tracks_df, audio_features)\n",
    "visualize_distributions(tracks_df, audio_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Correlations and Redundancy\n",
    "\n",
    "It's important to understand how our features relate to each other. Highly correlated features can disproportionately influence distance calculations in k-NN, essentially \"double-counting\" a single musical characteristic. We can visualize these relationships with a correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** (5 points): Analyze the correlation matrix generated by the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 1.2: Correlation Analysis (5 points)\n",
    "**Write your analysis below (150-200 words)**\n",
    "\n",
    "**Strong positive correlations (>0.5):** Which features show a strong positive correlation (e.g., > 0.5)? What musical relationship does this reveal (e.g., `energy` and `loudness`)?\n",
    "\n",
    "**Strong negative correlations (<-0.5):** Which features show a strong negative correlation (e.g., < -0.5)? What does this tell you (e.g., `acousticness` and `energy`)?\n",
    "\n",
    "**Double-counting concerns:**  Based on the heatmap, which pair of features are you most concerned about \"double counting\" a musical property when using a metric like Euclidean distance? Which feature pair would most bias k-NN and why?\n",
    "\n",
    "**Feature selection recommendation:** Should any features be removed? Justify\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = tracks_df[audio_features].corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            fmt='.2f')\n",
    "plt.title('Spotify Audio Feature Correlations', fontsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## Part 2: Implementing k-NN from Scratch (35 points)\n",
    "\n",
    "Now we implement the core recommendation algorithm. k-NN is particularly suitable for this music recommendation task because it is:\n",
    "1. **Instance-based**: No \"training\" is required. We can add new songs to our dataset instantly.\n",
    "2. **Interpretable**: Recommendations are explainable (e.g., \"This song is recommended because it is similar in energy and danceability to the one you like.\").\n",
    "3. **Flexible**: It works well with our custom, domain-specific distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Distance Metrics and k-NN Class Implementation\n",
    "\n",
    "**Question 3** (25 points): This is the core of the algorithm. You will implement the distance functions and the k-NN class in a single cell. This is good practice as it keeps related functionality together.\n",
    "\n",
    "1.  **Implement Distance Functions**: Complete `euclidean_distance` and `cosine_distance` as static methods within the class.\n",
    "2.  **Implement `find_neighbors`**: Complete the main k-NN logic in the `find_neighbors` method.\n",
    "\n",
    "**Deliverable**: Your completed `KNNRecommender` class and the output of the test cell showing it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL FOR STUDENT IMPLEMENTATION - KNN CLASS & DISTANCE FUNCTIONS\n",
    "class KNNRecommender:\n",
    "    \"\"\"A k-Nearest Neighbors recommender for music.\"\"\"\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "        self.item_profile = None\n",
    "        self.features_matrix = None\n",
    "        self.feature_columns = None\n",
    "        self.track_id_to_index = {}\n",
    "        \n",
    "    @staticmethod\n",
    "    def euclidean_distance(a, b):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean distance between two vectors.\n",
    "        \n",
    "        Args:\n",
    "            a (np.ndarray): Vector of shape (n,)\n",
    "            b (np.ndarray): Vector of shape (n,)\n",
    "            \n",
    "        Returns:\n",
    "            float: Euclidean distance\n",
    "            \n",
    "        Example:\n",
    "            >>> euclidean_distance(np.array([1,2,3]), np.array([4,5,6]))\n",
    "            5.196152422706632\n",
    "        \"\"\"\n",
    "        # --- YOUR IMPLEMENTATION GOES HERE ---\n",
    "        # --- SOLUTION ---\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_distance(a, b):\n",
    "        \"\"\"\n",
    "        Calculates the Cosine distance between two numerical vectors a and b.\n",
    "        Formula: 1 - (a¬∑b) / (||a|| * ||b||)\n",
    "                \n",
    "        Args:\n",
    "            a (np.ndarray): Vector of shape (n,)\n",
    "            b (np.ndarray): Vector of shape (n,)\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine distance (between 0 and 1)\n",
    "            \n",
    "        Example:\n",
    "            >>> cosine_distance(np.array([1,2,3]), np.array([4,5,6]))\n",
    "            0.025368153802923787\n",
    "            \n",
    "        Note: Return 1.0 if either vector has zero norm.\n",
    "        \"\"\"\n",
    "        # --- YOUR IMPLEMENTATION GOES HERE ---\n",
    "        # --- SOLUTION ---\n",
    "        pass\n",
    "        \n",
    "    def fit(self, item_profile_df, feature_columns):\n",
    "        \"\"\"Prepares the recommender by loading and processing the track data.\"\"\"\n",
    "        self.item_profile = item_profile_df.reset_index(drop=True)\n",
    "        self.feature_columns = feature_columns\n",
    "        self.features_matrix = self.item_profile[self.feature_columns].values\n",
    "        self.track_id_to_index = {track_id: i for i, track_id in enumerate(self.item_profile['id'])}\n",
    "        print(f\"Fit complete. Loaded {len(self.item_profile)} tracks.\")\n",
    "\n",
    "    def find_neighbors(self, track_id, n_neighbors=None, distance_metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Find k nearest neighbors for a track.\n",
    "        \n",
    "        Args:\n",
    "            track_id (str): Query track ID\n",
    "            n_neighbors (int): Number of neighbors (default: self.k)\n",
    "            distance_metric (str): 'euclidean' or 'cosine'\n",
    "            \n",
    "        Returns:\n",
    "            list: [(distance, track_id), ...] sorted by distance\n",
    "            \n",
    "        Example:\n",
    "            >>> neighbors = recommender.find_neighbors('track123', n_neighbors=5)\n",
    "            [(0.23, 'track456'), (0.31, 'track789'), ...]\n",
    "        \"\"\"\n",
    "        if n_neighbors is None: n_neighbors = self.k\n",
    "        distance_functions = {'euclidean': self.euclidean_distance, 'cosine': self.cosine_distance}\n",
    "        if distance_metric not in distance_functions: raise ValueError(f\"Unknown metric: {distance_metric}\")\n",
    "        if track_id not in self.track_id_to_index: raise ValueError(f\"Track ID {track_id} not found.\")\n",
    "\n",
    "        # --- YOUR IMPLEMENTATION GOES HERE ---\n",
    "        # --- SOLUTION ---\n",
    "        # TODO: Implement k-NN search\n",
    "        # Don't include the query track itself in results\n",
    "        pass\n",
    "\n",
    "    def recommend(self, track_id, n_recommendations=None, distance_metric='euclidean'):\n",
    "        if self.item_profile is None: raise RuntimeError(\"Recommender has not been fitted.\")\n",
    "        neighbors = self.find_neighbors(track_id, n_recommendations, distance_metric)\n",
    "        neighbor_ids = [tid for distance, tid in neighbors]\n",
    "        results_df = self.item_profile[self.item_profile['id'].isin(neighbor_ids)].copy()\n",
    "        distances_map = {tid: dist for dist, tid in neighbors}\n",
    "        results_df['distance'] = results_df['id'].map(distances_map)\n",
    "        return results_df.sort_values('distance')\n",
    "\n",
    "# --- Test your implementation ---\n",
    "print(\"Testing your distance functions from the class...\")\n",
    "a = np.array([1, 2, 3]); b = np.array([4, 5, 6]); z = np.zeros(3)\n",
    "assert np.isclose(KNNRecommender.euclidean_distance(a, b), 5.196152), \"Euclidean test failed.\"\n",
    "assert np.isclose(KNNRecommender.cosine_distance(a, b), 0.025368), \"Cosine test failed.\"\n",
    "print(\"‚úÖ Distance functions appear correct!\")\n",
    "\n",
    "print(\"\\nTesting your KNNRecommender class...\")\n",
    "recommender = KNNRecommender(k=5)\n",
    "recommender.fit(tracks_df.head(100), feature_columns=audio_features)\n",
    "neighbors = recommender.find_neighbors(tracks_df.iloc[0]['id'], distance_metric='cosine')\n",
    "assert len(neighbors) == 5, f\"Expected 5 neighbors, got {len(neighbors)}\"\n",
    "assert isinstance(neighbors[0], tuple), \"Neighbors should be a list of (distance, track_id) tuples\"\n",
    "assert neighbors[0][0] < neighbors[1][0], \"Neighbors should be sorted by distance\"\n",
    "print(\"‚úÖ KNNRecommender class appears correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 2: Implementation Analysis (10 points)\n",
    "**Document your implementation decisions**\n",
    "\n",
    "**Distance metric comparison:**\n",
    "- Euclidean distance is best for: [your explanation]\n",
    "- Cosine distance is best for: [your explanation]\n",
    "- For music recommendation, I prefer: [which and why]\n",
    "\n",
    "**Computational complexity:**\n",
    "- Time complexity of find_neighbors: O([your answer]) because [explanation]\n",
    "- Space complexity: O([your answer]) because [explanation]\n",
    "- For n=1,000,000 songs, estimated query time: [your calculation]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## Part 3: The Importance of Feature Scaling (10 points)\n",
    "\n",
    "### 3.1 The Problem with Unscaled Features\n",
    "\n",
    "As we saw in our data analysis, features like `tempo` have a much larger range than features like `danceability`. Let's see how this dramatically biases our recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distance_contribution(item_profile, features, track_a_id, track_b_id):\n",
    "    \"\"\"Calculates and visualizes which features contribute most to the Euclidean distance.\"\"\"\n",
    "    track_a = item_profile[item_profile['id'] == track_a_id][features].iloc[0]\n",
    "    track_b = item_profile[item_profile['id'] == track_b_id][features].iloc[0]\n",
    "\n",
    "    diff = track_a - track_b\n",
    "    squared_diff = diff ** 2\n",
    "    contribution_percent = (squared_diff / squared_diff.sum() * 100).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nContribution of each feature to Euclidean distance between track {track_a_id} and {track_b_id}:\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    contribution_percent.plot(kind='bar')\n",
    "    plt.title('Feature Contribution to Euclidean Distance (%)', fontsize=16)\n",
    "    plt.ylabel('Contribution (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get recommendations for a song using unscaled features and Euclidean distance\n",
    "unscaled_recommender = KNNRecommender(k=5)\n",
    "unscaled_recommender.fit(tracks_df, feature_columns=audio_features)\n",
    "\n",
    "query_song = tracks_df[tracks_df['artist'] == 'Daft Punk'].iloc[0]\n",
    "query_track_id = query_song['id']\n",
    "\n",
    "print(f\"Generating UNSCALED recommendations for '{query_song['song']}' by {query_song['artist']}...\")\n",
    "unscaled_recs = unscaled_recommender.recommend(query_track_id, distance_metric='euclidean')\n",
    "display(unscaled_recs[['id', 'song', 'artist', 'distance'] + audio_features])\n",
    "\n",
    "# Use helper to show which feature contributed most to the distance\n",
    "display_distance_contribution(tracks_df, audio_features, query_track_id, unscaled_recs.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: Notice how the recommended songs have a `tempo` very close to the query song's tempo, while other features like `valence` and `energy` can be wildly different. The bar chart confirms it: `tempo` is contributing over 99% of the distance! The other features are effectively being ignored.\n",
    "\n",
    "### 3.2 Implementing a Feature Scaler\n",
    "\n",
    "**Question 5** (10 points): To solve this, we must scale our features. We will use **Standard Scaling**, which rescales features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Your Task**: Implement the `FeatureScaler` class below.\n",
    "\n",
    "**Deliverable**: Your completed `FeatureScaler` implementation and the output of the test cell showing it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CELL FOR STUDENT IMPLEMENTATION - FEATURE SCALER\n",
    "\n",
    "class FeatureScaler:\n",
    "    \"\"\"A class to scale numerical features using Standard Scaling.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Learn mean and std for each feature in X.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Shape (n_samples, n_features)\n",
    "            \n",
    "        Sets:\n",
    "            self.mean: Mean per feature\n",
    "            self.std: Std per feature (replace 0 with 1)\n",
    "        \"\"\"\n",
    "        # --- SOLUTION ---\n",
    "        # TODO: Implement\n",
    "        # NOTE: Make sure you set the values of self.mean and self.std\n",
    "        # No need to return any values\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply scaling: (X - mean) / std\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Scaled data, same shape as X\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If not fitted yet\n",
    "        \"\"\"\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise RuntimeError(\"Scaler has not been fitted yet. Call fit() first.\")\n",
    "        # --- SOLUTION ---\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "            \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"A convenience method to fit and transform in one step.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# --- Test your implementation ---\n",
    "print(\"Testing your FeatureScaler...\")\n",
    "X_test = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]], dtype=float)\n",
    "scaler = FeatureScaler()\n",
    "X_scaled = scaler.fit_transform(X_test)\n",
    "assert np.allclose(X_scaled.mean(axis=0), [0, 0]), \"Scaled mean should be 0.\"\n",
    "assert np.allclose(X_scaled.std(axis=0), [1, 1]), \"Scaled std dev should be 1.\"\n",
    "print(\"‚úÖ FeatureScaler class appears correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparing Results with Scaled Features\n",
    "Now, let's re-run our recommendation using the scaled features and see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the unscaled feature matrix from our main dataframe\n",
    "unscaled_features_matrix = tracks_df[audio_features].values\n",
    "\n",
    "# 2. Use your scaler to transform the features\n",
    "scaler = FeatureScaler()\n",
    "scaled_features_matrix = scaler.fit_transform(unscaled_features_matrix)\n",
    "\n",
    "# 3. Create a new DataFrame with the scaled features for the recommender\n",
    "scaled_tracks_df = tracks_df.copy()\n",
    "scaled_tracks_df[audio_features] = scaled_features_matrix\n",
    "\n",
    "# 4. Fit a new recommender with the SCALED data\n",
    "scaled_recommender = KNNRecommender(k=5)\n",
    "scaled_recommender.fit(scaled_tracks_df, feature_columns=audio_features)\n",
    "\n",
    "# 5. Get recommendations for the same song\n",
    "# print(f\"Generating SCALED recommendations for '{query_song['song']}' by {query_song['artist']}...\")\n",
    "# scaled_recs_df = scaled_recommender.recommend(query_track_id, distance_metric='euclidean')\n",
    "\n",
    "# # --- Display original, unscaled features for clarity ---\n",
    "# rec_ids = scaled_recs_df['id'].tolist()\n",
    "# display_df = tracks_df[tracks_df['id'].isin(rec_ids)].copy()\n",
    "# display_df['distance'] = display_df['id'].map(dict(zip(scaled_recs_df['id'], scaled_recs_df['distance'])))\n",
    "# display(display_df[['song', 'artist', 'distance'] + audio_features].sort_values('distance'))\n",
    "\n",
    "\n",
    "# Get recommendations using scaled data\n",
    "print(f\"Generating SCALED recommendations for '{query_song['song']}' by {query_song['artist']}...\")\n",
    "scaled_recs_df = scaled_recommender.recommend(query_track_id, distance_metric='euclidean')\n",
    "\n",
    "# The recommendations have IDs and distances, but scaled features\n",
    "# We want to show original features with the distances from scaled calculation\n",
    "rec_ids = scaled_recs_df['id'].values\n",
    "distances = scaled_recs_df['distance'].values\n",
    "\n",
    "# Get original unscaled data for these tracks\n",
    "original_recs = tracks_df[tracks_df['id'].isin(rec_ids)].copy()\n",
    "\n",
    "# Add the distances\n",
    "distance_map = dict(zip(rec_ids, distances))\n",
    "original_recs['distance'] = original_recs['id'].map(distance_map)\n",
    "\n",
    "# Sort and display\n",
    "original_recs = original_recs.sort_values('distance')\n",
    "display(original_recs[['song', 'artist', 'distance'] + audio_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 3: Scaling Impact (10 points)\n",
    "**Analyze the effect of feature scaling** How did the recommendations change after scaling? Do they seem more musically relevant now? Why?\n",
    "\n",
    "**Before scaling:**\n",
    "- Dominant feature(s): [which and why]\n",
    "- Recommendation bias: [what type of songs were favored]\n",
    "\n",
    "**After scaling:**\n",
    "- Feature contribution is now: [balanced/improved - explain]\n",
    "- Recommendation quality: [how did it change]\n",
    "\n",
    "**Standard vs Min-Max scaling:** In what situations might you prefer Min-Max scaling over Standard scaling, or vice-versa? (Hint: think about outliers and the desired output distribution).\n",
    "- Use Standard scaling when: [your answer]\n",
    "- Use Min-Max scaling when: [your answer]\n",
    "- For this dataset, better choice is: [which and why]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## Part 4: Advanced Similarity Metrics & Complexity (20 points)\n",
    "\n",
    "### 4.1 Computational Complexity Analysis\n",
    "\n",
    "**Question 6** (5 points): A critical aspect of applied ML is analyzing scalability. How will our recommender perform as the number of songs grows? Our current implementation is a \"brute-force\" search, meaning it calculates the distance to every single other song for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scaling_behavior(df, features, recommender_class):\n",
    "    \"\"\"Measures how query time scales with the size of the dataset.\"\"\"\n",
    "    sizes = [100, 500, 1000, 2500, 5000, 10000]\n",
    "    results = []\n",
    "    for size in sizes:\n",
    "        if size > len(df): break\n",
    "        print(f\"Testing with dataset size: {size}...\")\n",
    "        knn = recommender_class(k=10)\n",
    "        subset = df.head(size).copy()\n",
    "        knn.fit(subset, features)\n",
    "        query_times = []\n",
    "        n_queries = min(20, size)\n",
    "        query_indices = np.random.choice(size, n_queries, replace=False)\n",
    "        for idx in query_indices:\n",
    "            track_id = subset.iloc[idx]['id']\n",
    "            start_time = time.time()\n",
    "            _ = knn.find_neighbors(track_id, n_neighbors=10)\n",
    "            end_time = time.time()\n",
    "            query_times.append(end_time - start_time)\n",
    "        results.append({'size': size, 'mean_time_s': np.mean(query_times), 'std_time_s': np.std(query_times)})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_scaling_with_uncertainty(results_df):\n",
    "    \"\"\"Plots query time vs. dataset size with error bars on a log-log scale.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.errorbar(results_df['size'], results_df['mean_time_s'], yerr=results_df['std_time_s'], marker='o', linestyle='-', capsize=5, label='Measured Query Time')\n",
    "    sizes = results_df['size'].values\n",
    "    theoretical_line = results_df['mean_time_s'].iloc[0] * (sizes / sizes[0])\n",
    "    ax.plot(sizes, theoretical_line, '--', color='red', label='Theoretical O(n) Scaling')\n",
    "    ax.set_xlabel('Dataset Size (Number of Tracks)', fontsize=12)\n",
    "    ax.set_ylabel('Average Query Time (seconds)', fontsize=12)\n",
    "    ax.set_title('k-NN Brute-Force Search: Query Time Scaling', fontsize=16)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This may take a minute to run\n",
    "scaling_results = measure_scaling_behavior(df=scaled_tracks_df, features=audio_features, recommender_class=KNNRecommender)\n",
    "plot_scaling_with_uncertainty(scaling_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 4.1: Scalability Analysis (5 points)\n",
    "**Based on your complexity measurements**\n",
    "\n",
    "**Performance observations:**\n",
    "- Does your implementation's performance curve closely follow the theoretical O(n) line? [yes/no and evidence]\n",
    "- Based on your plot, extrapolate to estimate the query time for 1,000,000 songs: [your extrapolation] seconds\n",
    "- Is this latency (e.g., >1 second) acceptable for a real-time web application? [yes/no and why]\n",
    "\n",
    "**Optimization needed:** What does this analysis tell you about the limitations of a \"brute-force\" k-NN search? What kind of optimizations (e.g., Approximate Nearest Neighbor methods like LSH or HNSW) would be necessary for a real-world application at scale?\n",
    "- Current bottleneck: [what takes most time]\n",
    "- Suggested optimization: [e.g., LSH, HNSW, explain briefly]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Designing a Hybrid Distance Function\n",
    "\n",
    "**Question 7** (15 points): Real-world recommendation systems often use **hybrid** functions that combine multiple signals to better capture user intent. This is where the \"art\" of machine learning comes in. \n",
    "\n",
    "**Your Task**: Design and implement a `custom_hybrid_distance` function. This is an open-ended task. Your goal is to create a function that you believe is more musically intelligent than a simple audio feature distance. You can combine audio similarity with metadata like artist, genre, or release year. \n",
    "\n",
    "Some ideas from research to get you started:\n",
    "1. **Linear Combination**: `D_hybrid = w1 * D_audio + w2 * D_artist_penalty`\n",
    "2. **Hierarchical Filtering**: `if same_artist: return D_audio * 0.5 else: return D_audio`\n",
    "3. **Context-Adaptive Weights**: Create different weights for different situations (e.g., a 'workout' context might put more weight on `energy` and `tempo`).\n",
    "\n",
    "For this assignment, a weighted linear combination is a great approach. You will implement your custom function, integrate it into a new `HybridKNNRecommender` class, and test its performance.\n",
    "\n",
    "**Deliverable**: Your completed `custom_hybrid_distance` function and `HybridKNNRecommender` class, and the output of the test cell comparing recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 4.2: Hybrid Distance Design (15 points)\n",
    "**Explain your custom distance function**\n",
    "\n",
    "**Design rationale:**\n",
    "- Components used: [list them]\n",
    "- Weight choices: [explain your weights]\n",
    "- Musical intuition: [what behavior were you trying to model]\n",
    "\n",
    "**Comparison with basic metrics:**\n",
    "- Improvements observed: [what got better]\n",
    "- Trade-offs: [what got worse, if anything]\n",
    "\n",
    "**Use cases:**\n",
    "- This hybrid function is best for: [scenario]\n",
    "- It would not work well for: [scenario]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pass the full data rows now, in addition to the feature vectors\n",
    "def custom_hybrid_distance(track_a_data, track_b_data, audio_features_a, audio_features_b, w_artist=0.5):\n",
    "    \"\"\"\n",
    "    Design your hybrid distance function here.\n",
    "    \n",
    "    Args:\n",
    "        track_a_data (pd.Series): Full data row for track A\n",
    "        track_b_data (pd.Series): Full data row for track B\n",
    "        audio_features_a (np.ndarray): Audio feature vector for track A\n",
    "        audio_features_b (np.ndarray): Audio feature vector for track B\n",
    "        w_artist (float): Weight for metadata component (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        float: Combined distance value\n",
    "        \n",
    "    Ideas to consider:\n",
    "        - Audio similarity (using cosine or euclidean distance)\n",
    "        - Artist similarity (same artist = lower distance)\n",
    "        - You could also consider: genre, year, popularity, etc.\n",
    "    \"\"\"\n",
    "    # TODO: Implement your hybrid distance\n",
    "    pass\n",
    "\n",
    "\n",
    "class HybridKNNRecommender(KNNRecommender):\n",
    "    def find_neighbors(self, track_id, n_neighbors=None, distance_metric='hybrid', w_artist=0.5):\n",
    "        \"\"\"\n",
    "        Find neighbors using hybrid distance that combines audio features and metadata.\n",
    "        \n",
    "        This method extends the base KNNRecommender to use the custom_hybrid_distance\n",
    "        function when distance_metric='hybrid'.\n",
    "        \"\"\"\n",
    "        if distance_metric != 'hybrid':\n",
    "            return super().find_neighbors(track_id, n_neighbors, distance_metric)\n",
    "        \n",
    "        if n_neighbors is None: \n",
    "            n_neighbors = self.k\n",
    "            \n",
    "        if track_id not in self.track_id_to_index: \n",
    "            raise ValueError(f\"Track ID {track_id} not found.\")\n",
    "        \n",
    "        # --- YOUR IMPLEMENTATION GOES HERE ---\n",
    "        # TODO: Implement hybrid k-NN search\n",
    "        # 1. Get query track's features and metadata\n",
    "        # 2. For each other track:\n",
    "        #    - Calculate hybrid distance using custom_hybrid_distance\n",
    "        #    - Store (distance, track_id)\n",
    "        # 3. Sort and return top n_neighbors\n",
    "        pass\n",
    "    \n",
    "\n",
    "# --- Test your implementation ---\n",
    "def get_rec_metadata(rec_tuples, tracks_df):\n",
    "    \"\"\"Helper to get full metadata for recommendation tuples.\"\"\"\n",
    "    rec_data = []\n",
    "    for dist, tid in rec_tuples:\n",
    "        track = tracks_df[tracks_df['id'] == tid]\n",
    "        if not track.empty:\n",
    "            rec_data.append({\n",
    "                'song': track.iloc[0]['song'],\n",
    "                'artist': track.iloc[0]['artist'],\n",
    "                'distance': dist\n",
    "            })\n",
    "    return pd.DataFrame(rec_data)\n",
    "    \n",
    "# Initialize and fit the hybrid recommender\n",
    "hybrid_recommender = HybridKNNRecommender(k=5)\n",
    "hybrid_recommender.fit(scaled_tracks_df, feature_columns=audio_features)\n",
    "\n",
    "query_song = tracks_df[tracks_df['artist'] == 'Daft Punk'].iloc[0]\n",
    "query_track_id = query_song['id']\n",
    "\n",
    "# Get recommendations favoring variety\n",
    "print(f\"HYBRID recommendations for '{query_song['song']}' (favoring artist variety, w_artist=0.1)...\")\n",
    "hybrid_recs_variety = hybrid_recommender.find_neighbors(query_track_id, w_artist=0.1)\n",
    "display(get_rec_metadata(hybrid_recs_variety, tracks_df))\n",
    "\n",
    "# Get recommendations favoring the same artist\n",
    "print(f\"\\nHYBRID recommendations for '{query_song['song']}' (favoring same artist, w_artist=0.9)...\")\n",
    "hybrid_recs_same = hybrid_recommender.find_neighbors(query_track_id, w_artist=0.9)\n",
    "display(get_rec_metadata(hybrid_recs_same, tracks_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a>\n",
    "## Part 5: From Model to Web Application (10 points)\n",
    "\n",
    "A model in a notebook is a great start, but the goal of most projects is to deploy it as part of a live application.\n",
    "\n",
    "### 5.1 API and Caching Strategy\n",
    "\n",
    "**Question 8** (10 points): An API (Application Programming Interface) is how the web frontend communicates with our Python backend. Caching is vital because many users might request recommendations for the same popular songs, and we don't want to re-calculate them every time.\n",
    "\n",
    "**Deliverable**: Review the proposed API and Caching strategy below. In a separate document, explain *why* this design is effective. Discuss the role of the cache key, what is being cached, the cache size, and the TTL (Time-To-Live). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù REPORT SECTION 5: Production Design (10 points)\n",
    "**Your API and caching strategy**\n",
    "\n",
    "**API Design:**\n",
    "Route: [your route]\n",
    "Method: [GET/POST]\n",
    "Parameters: [list them]\n",
    "Response format: [show example JSON]\n",
    "\n",
    "**Caching Strategy:**\n",
    "- Cache key: [how you generate it]\n",
    "- Cache size: [number] entries\n",
    "- TTL: [time] because [justification]\n",
    "- Eviction policy: [LRU/LFU] because [justification]\n",
    "\n",
    "**Production considerations:**\n",
    "- Expected QPS: [your estimate]\n",
    "- Latency requirement: [target in ms]\n",
    "- Scalability plan: [brief description]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Endpoint Specification\n",
    "\n",
    "- **Route**: `/api/recommend/track/<string:track_id>`\n",
    "- **HTTP Method**: `GET`\n",
    "- **Query Parameters**:\n",
    "  - `k`: (integer, optional, default: 10) Number of recommendations.\n",
    "  - `metric`: (string, optional, default: 'cosine') The distance metric.\n",
    "- **Example Success Response (200 OK)**:\n",
    "  ```json\n",
    "  {\n",
    "    \"success\": true,\n",
    "    \"data\": {\n",
    "      \"rec_track_1\": {\"song\": \"Song A\", \"artist\": \"Artist X\", ...},\n",
    "      \"rec_track_2\": {\"song\": \"Song B\", \"artist\": \"Artist Y\", ...}\n",
    "    }\n",
    "  }\n",
    "  ```\n",
    "\n",
    "#### Caching Strategy\n",
    "\n",
    "We would implement a **Least Recently Used (LRU) in-memory cache**.\n",
    "\n",
    "- **Cache Key**: A unique key generated from the request: `f\"{track_id}_{k}_{metric}\"`.\n",
    "- **What is Cached**: The final JSON response object.\n",
    "- **Cache Size & Eviction**: Store, for example, 10,000 recent results. When full, discard the least recently used item.\n",
    "- **Cache TTL (Time-To-Live)**: Results expire after 1 hour to allow the system to adapt to new songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Exporting Your Final Code for the Web App\n",
    "This is the final and most crucial step. Our Flask web server needs to use the code you just wrote.\n",
    "\n",
    "To complete the web integration, you need to manually copy your implementations to `utils/student_adapter.py`. Here's exactly what to copy:\n",
    "\n",
    "#### Step 1: Copy FeatureScaler Class\n",
    "- **From**: The cell where you implemented `FeatureScaler` (Question 5, Section 3.2)\n",
    "- **To**: `utils/student_adapter.py` starting at line 19\n",
    "- **Replace**: The entire `FeatureScaler` class including all TODOs\n",
    "\n",
    "#### Step 2: Copy KNNRecommender Class  \n",
    "- **From**: The cell where you implemented `KNNRecommender` (Question 3, Section 2.1)\n",
    "- **To**: `utils/student_adapter.py` starting at line 44\n",
    "- **What to include**:\n",
    "  - The complete `__init__` method\n",
    "  - Both static methods: `euclidean_distance` and `cosine_distance`\n",
    "  - The `fit` method\n",
    "  - The `find_neighbors` method  \n",
    "  - The `recommend` method\n",
    "- **Replace**: The entire `KNNRecommender` class including all TODOs\n",
    "\n",
    "#### Step 3: (Optional) Copy HybridKNNRecommender\n",
    "- **From**: The cell where you implemented `HybridKNNRecommender` (Question 7, Section 4.2)\n",
    "- **To**: `utils/student_adapter.py` starting at line 89\n",
    "- **Only if**: You completed the hybrid implementation\n",
    "\n",
    "#### Step 4: Test Your Implementation\n",
    "After copying, run this in terminal from the project root:\n",
    "```bash\n",
    "python -m utils.test_student_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Final Steps for Web Integration\n",
    "\n",
    "### Test Your Implementation\n",
    "Run this cell to verify your code is ready for the web app:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final implementation test\n",
    "print(\"Testing all components...\")\n",
    "\n",
    "# Test 1: Distance functions\n",
    "test_a = np.array([1, 2, 3])\n",
    "test_b = np.array([4, 5, 6])\n",
    "assert abs(KNNRecommender.euclidean_distance(test_a, test_b) - 5.196) < 0.01, \"Euclidean failed\"\n",
    "assert abs(KNNRecommender.cosine_distance(test_a, test_b) - 0.025) < 0.01, \"Cosine failed\"\n",
    "print(\"‚úÖ Distance functions work\")\n",
    "\n",
    "# Test 2: Scaler\n",
    "test_data = np.array([[1, 10], [2, 20], [3, 30]])\n",
    "scaler_test = FeatureScaler()\n",
    "scaled_test = scaler_test.fit_transform(test_data)\n",
    "assert abs(scaled_test.mean()) < 0.01, \"Scaling failed\"\n",
    "print(\"‚úÖ FeatureScaler works\")\n",
    "\n",
    "# Test 3: KNN Recommender\n",
    "test_recommender = KNNRecommender(k=3)\n",
    "test_df = scaled_tracks_df.head(100)\n",
    "test_recommender.fit(test_df, audio_features)\n",
    "test_neighbors = test_recommender.find_neighbors(test_df.iloc[0]['id'])\n",
    "assert len(test_neighbors) == 3, \"KNN failed\"\n",
    "print(\"‚úÖ KNNRecommender works\")\n",
    "\n",
    "print(\"\\nüéâ All tests passed! Now copy your implementations to utils/student_adapter.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install flask flask-cors\n",
    "%pip -q install -U yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Flask server startup with proper error handling and output display\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# free a port, replace 5002 if needed.\n",
    "import os, signal, subprocess\n",
    "for pid in subprocess.getoutput(\"lsof -ti:5002 -sTCP:LISTEN\").split():\n",
    "    try: os.kill(int(pid), signal.SIGTERM)\n",
    "    except ProcessLookupError: pass\n",
    "        \n",
    "server_process = None\n",
    "output_thread = None\n",
    "\n",
    "def stream_output(process):\n",
    "    \"\"\"Continuously read and display server output.\"\"\"\n",
    "    try:\n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            if line:\n",
    "                print(line.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Output streaming error: {e}\")\n",
    "\n",
    "def start_server():\n",
    "    global server_process, output_thread\n",
    "    \n",
    "    # Kill any existing server\n",
    "    if server_process and server_process.poll() is None:\n",
    "        print(\"Stopping existing server...\")\n",
    "        try:\n",
    "            server_process.terminate()\n",
    "            server_process.wait(timeout=5)\n",
    "        except:\n",
    "            server_process.kill()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Starting Flask web server...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the directory of the notebook (assumes notebook is in project root)\n",
    "    notebook_dir = os.getcwd()\n",
    "    \n",
    "    # Check if app.py exists\n",
    "    if not os.path.exists('app.py'):\n",
    "        print(\"‚ùå ERROR: app.py not found in current directory!\")\n",
    "        print(f\"Current directory: {notebook_dir}\")\n",
    "        print(\"Make sure you're running the notebook from the project root directory.\")\n",
    "        return\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['utils/student_adapter.py', 'data/item_profile.csv']\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "    if missing_files:\n",
    "        print(\"‚ö†Ô∏è WARNING: Missing files:\")\n",
    "        for f in missing_files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    try:\n",
    "        # Start the server with combined stdout/stderr for easier debugging\n",
    "        server_process = subprocess.Popen(\n",
    "            [sys.executable, 'app.py'],  # use the current kernel's interpreter\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,  # Combine stderr with stdout\n",
    "            text=True,\n",
    "            bufsize=1,  # Line buffered\n",
    "            cwd=notebook_dir  # Explicitly set working directory\n",
    "        )\n",
    "        \n",
    "        # Start thread to continuously display output\n",
    "        output_thread = threading.Thread(target=stream_output, args=(server_process,))\n",
    "        output_thread.daemon = True\n",
    "        output_thread.start()\n",
    "        \n",
    "        # Give server time to start and check if it's running\n",
    "        time.sleep(3)\n",
    "        \n",
    "        if server_process.poll() is not None:\n",
    "            # Process ended\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"‚ùå Server failed to start!\")\n",
    "            print(\"Check the error messages above.\")\n",
    "            print(\"=\"*60)\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"‚úÖ Server appears to be running!\")\n",
    "            print(\"üåê Navigate to: http://127.0.0.1:5002\")\n",
    "            print(\"üìù Server output will appear below...\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to start server: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Make sure Flask is installed: pip install flask flask-cors\")\n",
    "        print(\"2. Check that you've copied your code to utils/student_adapter.py\")\n",
    "        print(\"3. Try running directly in terminal: python app.py\")\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"Stop the Flask server.\"\"\"\n",
    "    global server_process\n",
    "    if server_process and server_process.poll() is None:\n",
    "        print(\"\\nStopping Flask server...\")\n",
    "        try:\n",
    "            # Try graceful termination first\n",
    "            server_process.terminate()\n",
    "            server_process.wait(timeout=5)\n",
    "            print(\"‚úÖ Server stopped.\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            # Force kill if graceful termination fails\n",
    "            server_process.kill()\n",
    "            print(\"‚úÖ Server forcefully stopped.\")\n",
    "    else:\n",
    "        print(\"No server is currently running.\")\n",
    "\n",
    "# Register cleanup on notebook shutdown\n",
    "import atexit\n",
    "atexit.register(stop_server)\n",
    "\n",
    "# Start the server\n",
    "start_server()\n",
    "\n",
    "# Note: You can call stop_server() manually if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part7'></a>\n",
    "## Part 6: Beyond the Basics: Research Frontiers\n",
    "\n",
    "### 6.1 Learning Personalized Distance Metrics\n",
    "\n",
    "Modern recommendation systems learn from user behavior to personalize distance metrics. Rather than using fixed weights, they adapt to individual preferences through implicit and explicit feedback. The mathematical framework for this, often called *Metric Learning*, treats distance learning as an optimization problem. Given user feedback in the form of preferred and non-preferred items (e.g., triplets like `(anchor, positive_example, negative_example)`), the system learns a transformation of the feature space (a matrix **M**) to minimize a ranking loss, such that the distance `d(anchor, positive)` is smaller than `d(anchor, negative)`.\n",
    "\n",
    "The challenge lies in balancing personalization with exploration. Too much personalization creates filter bubbles where users only see similar content. Real-world systems address this through multi-armed bandit algorithms that balance exploiting known preferences with exploring new areas. Privacy is also paramount; federated learning and differential privacy are used to learn from user data without it ever leaving their device.\n",
    "\n",
    "### 6.2 Evaluation Beyond Accuracy\n",
    "\n",
    "While accuracy is important, real-world systems optimize for multiple objectives that better reflect user satisfaction and business goals.\n",
    "\n",
    "- **Diversity**: Measures the variety within a recommendation list, often calculated as the average pairwise distance between recommended items. The challenge is balancing diversity with relevance.\n",
    "- **Novelty & Serendipity**: Novelty measures how new recommendations are (often approximated by inverse item popularity). Serendipity measures how surprisingly relevant they are. These are crucial for long-term engagement.\n",
    "- **Coverage**: Examines what fraction of the item catalog gets recommended. Low coverage means the system only recommends popular items, creating a \"rich get richer\" effect that's unfair to new artists.\n",
    "\n",
    "These metrics often conflict. Optimizing for accuracy might reduce diversity. Production systems use multi-objective optimization techniques, often validated through extensive A/B testing that measures long-term user engagement and retention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a music recommendation system from the ground up, moving from fundamental data analysis to advanced, production-oriented concepts. You've implemented and tested the k-NN algorithm, designed custom similarity metrics, analyzed computational complexity, and planned for real-world deployment.\n",
    "\n",
    "The techniques you've learned here‚Äîdata exploration, feature engineering, model implementation, performance analysis, and system design‚Äîare the core building blocks of applied machine learning. Whether you're building recommendation systems for e-commerce, content platforms, or social networks, these principles remain the same: understand your data deeply, design thoughtful models, validate rigorously, and engineer for a larger application context.\n",
    "\n",
    "## Reflection\n",
    "**Write a brief reflection (200-300 words) on:**\n",
    "- What was the most challenging part of this project?\n",
    "- What insights did you gain about recommendation systems?\n",
    "- How would you extend this system for a real production environment?\n",
    "- What did you learn about the gap between ML algorithms and deployed applications?\n",
    "\n",
    "---\n",
    "**END OF ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgment:** This notebook draws conceptual inspiration from RUMusic (https://github.com/vraj152/RUMusic), though our implementation and website have significantly diverged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (music-py311)",
   "language": "python",
   "name": "music-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
